{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zixuan Chen 61665307"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter as clock\n",
    "from beautifultable import BeautifulTable\n",
    "np.random.seed(0)\n",
    "from hw4_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content of hw4_helper\n",
    "My Funtion Object implemented in Homework2 and Homework3. The differetiation method is implemented using Sympy library.\\\n",
    "My Conjugate Gradient method implemented in Homework2 using backtracking line search. It will be used in Problem 1.\\\n",
    "$$-Source:Kochenderfer \\& Wheeler book, Algorithm 4.2 (pg.56), \"backtracking\\_line\\_search\".$$$$ Equation 5.16 (pg.73), \"Fletcher-Reeves\". Algorithm 5.2 (pg.74), \"ConjugateGradientDescent\".$$\n",
    "Helper Functions:\n",
    "$$array(*args, ndmin = 1) -> np.array$$\n",
    "$$gradient(f:Function) -> array(Function)$$\n",
    "$$compute\\_vector(v: [Function], x: np.array, ndmin = 1) -> np.array$$\n",
    "$$gradient\\_magnitude(g: [Function], x: np.array) -> float$$\n",
    "$$vector\\_length(v) -> float$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1  Constrained Optimization\n",
    "## Augmented Lagrange Method\n",
    "The method I will use for this problem is the augmented Lagrangian method.\n",
    "$$-Source: Kochenderfer and Wheeler book (pg.183) Algorithm 10.2, \"augmented\\_lagrange\\_method\".$$\n",
    "\"penalty_function\" will turn the penalty function into a Function object, then I can use its methods to take the gradient.\\\n",
    "The termination condition for Augmented Lagrange Method is \n",
    "$$||h(x)|| <= \\epsilon$$\n",
    "However, there are chances that the start pointing is carefully designed to make h(x) = 0 before reaching the optimum. To escape the carefully designed point, I add an additional condition that it will run for at least 5 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty_function(f, rho, h, x, lmda) -> (Function, [Function]):\n",
    "    '''x -> f(x) + rho / 2 * sum(compute_vector(h, x)**2) - lmda.dot(compute_vector(h, x))'''\n",
    "    if len(h) == 0:\n",
    "        return f, gradient(f)\n",
    "    f_str = f.func + '+' + str(rho/2) + '*((' \n",
    "    f_str += '+('.join([h_i.func +')**2' for h_i in h]) + ')-'\n",
    "    f_str += '-'.join([f'{lmda[i]}*({h[i]})' for i in range(len(h))])\n",
    "    p = Function(f_str, ','.join([f'x{i}' for i in range(len(x))]))\n",
    "    return p, gradient(p)            \n",
    "    \n",
    "def augmented_lagrange(f, f_prime, h, initial_x, rho = 1, gama = 2, tol = 1e-4):\n",
    "    start_time = clock()\n",
    "    x = initial_x.copy()\n",
    "    hx = compute_vector(h, x) # h(x)\n",
    "    lmda = np.zeros(len(hx))\n",
    "    iteration = 0\n",
    "    while vector_length(hx) > tol or iteration < 5:\n",
    "        iteration += 1\n",
    "        penalty, penalty_prime = penalty_function(f, rho, h, x, lmda)\n",
    "        x = conjugate_gradients(penalty, x, penalty_prime) #; print(x)\n",
    "        hx = compute_vector(h, x)\n",
    "        rho *= gama\n",
    "        lmda -= rho * hx\n",
    "    return x, vector_length(hx), f(x), iteration, clock() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test & Report\n",
    "Here I define two test functions:\n",
    "$$test\\_function1 = \\sum_{i=0}^{9} (x_{i}-1)^2$$\n",
    "$$test\\_function2 = \\sum_{i=0}^{8} (1-x_{i})^2 + 5(x_{i+1}-x_{i}^2)^2$$\n",
    "And two constraints:\n",
    "$$(inactive)h1 = \\sum_{i=0}^{9} x_{i} = 0 $$\n",
    "$$(active)h2 = x_0 - 2 = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Function 1: Quadratic\n",
      "(x0 - 1)**2 + (x1 - 1)**2 + (x2 - 1)**2 + (x3 - 1)**2 + (x4 - 1)**2 + (x5 - 1)**2 + (x6 - 1)**2 + (x7 - 1)**2 + (x8 - 1)**2 + (x9 - 1)**2\n",
      "variables: ['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']\n",
      "\n",
      "Test Function 2: Rosenbrock\n",
      "(1 - x0)**2 + 5*(x1 - x0**2)**2 + (1 - x1)**2 + 5*(x2 - x1**2)**2 + (1 - x2)**2 + 5*(x3 - x2**2)**2 + (1 - x3)**2 + 5*(x4 - x3**2)**2 + (1 - x4)**2 + 5*(x5 - x4**2)**2 + (1 - x5)**2 + 5*(x6 - x5**2)**2 + (1 - x6)**2 + 5*(x7 - x6**2)**2 + (1 - x7)**2 + 5*(x8 - x7**2)**2 + (1 - x8)**2 + 5*(x9 - x8**2)**2\n",
      "variables: ['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']\n",
      "\n",
      "Equality Constraint 1: \n",
      "x0 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 - 10 = 0\n",
      "variables: ['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']\n",
      "\n",
      "Equality Constraint 2: \n",
      "x0 - 2 = 0\n",
      "variables: ['x0']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10-dim (x-1)^2\n",
    "f_str = ' + '.join([f'(x{i} - 1)**2' for i in range(10)])\n",
    "test_func1 = Function(f_str, ','.join([f'x{i}' for i in range(10)]), name = 'Quadratic')\n",
    "\n",
    "# 10-dim Rosenbrock \n",
    "f_str = ' + '.join([f'(1 - x{i})**2 + 5*(x{i+1} - x{i}**2)**2' for i in range(9)])\n",
    "test_func2 = Function(f_str, ','.join([f'x{i}' for i in range(10)]), name = 'Rosenbrock')\n",
    "\n",
    "# Equality Constraints\n",
    "h1 = ' + '.join([f'x{i}' for i in range(10)]) + ' - 10'\n",
    "h1 = Function(h1, ','.join([f'x{i}' for i in range(10)]))\n",
    "h2 = Function('x0 - 2', 'x0')\n",
    "\n",
    "for i, tf in enumerate([test_func1, test_func2], 1):\n",
    "    print(f'Test Function {i}: {tf.name}\\n{tf}\\nvariables: {tf.symbols}\\n')\n",
    "for i, c in enumerate([h1, h2], 1):\n",
    "    print(f'Equality Constraint {i}: {c.name}\\n{c} = 0\\nvariables: {c.symbols}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_solution(solutions):\n",
    "    return set(', '.join(map(lambda x: str(round(x, 3)), s)) for s in solutions)\n",
    "    \n",
    "def test_problem1(test_func, func_prime, constraints, initial_points, sample_size):\n",
    "    result = np.array([augmented_lagrange(test_func, func_prime, constraints, initial_points[i]) for i in range(sample_size)])\n",
    "    solutions, result = result[:,0], result[:,1:]\n",
    "    \n",
    "    table = BeautifulTable()\n",
    "    table.set_style(BeautifulTable.STYLE_COMPACT)\n",
    "    header = test_func.name+' s.t.\\n'+'\\n'.join([str(c)+' = 0' for c in constraints])\n",
    "    table.column_headers = [header]\n",
    "    \n",
    "    report = BeautifulTable()\n",
    "    table.append_row([report])\n",
    "    report.column_headers = [\"||h(x)||\", \"f(x)\", \"Iteration\", \"time\"]\n",
    "    \n",
    "    row = []\n",
    "    for i in range(result.shape[1]):\n",
    "        m = np.mean(result[:,i])  # sample mean\n",
    "        s = np.sqrt(sum([(k - m)**2 for k in result[:,i]]) / (sample_size - 1)) # unbiased estimate of population std\n",
    "        row.append('mean:{:.3e}\\nstd:{:.3e}'.format(m,s))\n",
    "    report.append_row(row)\n",
    "    return sample_solution(solutions), table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method will be tested on 20 randomly selected initial points. Each is a 10-dimensional point in [-20, 20].\\\n",
    "There will also be two seperated tests. One uses only the inactive constraint, the other use both constraints. The first test result should look similar to reports from previous homeworks.\\\n",
    "Under the table, I also provide a sample solution, a non-repetitive set of solution given by the runs (rounded to three decimal places).\\\n",
    "For the report, there are four things I interested in:\\\n",
    "(1) ||h(x)||: How well the solution satisfied the constraints. This is expected to be very small.\\\n",
    "(2) f(x): Function values. Two test functions are designed to have a global mimunm at f(x) = 0. Therefore the closer to zero, the better the solution is.\\\n",
    "(3) Iteration and (4) Running time: Both indicate how efficient the method is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Quadratic s.t.                             \n",
      "       x0 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 - 10 = 0        \n",
      "-----------------------------------------------------------------------\n",
      " +----------------+----------------+----------------+----------------+ \n",
      " |    ||h(x)||    |      f(x)      |   Iteration    |      time      | \n",
      " +----------------+----------------+----------------+----------------+ \n",
      " | mean:1.416e-06 | mean:1.327e-12 | mean:5.000e+00 | mean:3.468e-01 | \n",
      " | std:8.169e-07  | std:2.817e-12  | std:0.000e+00  | std:4.651e-02  | \n",
      " +----------------+----------------+----------------+----------------+ \n",
      "Sample Solution:  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n",
      "\n",
      "                            Rosenbrock s.t.                            \n",
      "       x0 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 - 10 = 0        \n",
      "-----------------------------------------------------------------------\n",
      " +----------------+----------------+----------------+----------------+ \n",
      " |    ||h(x)||    |      f(x)      |   Iteration    |      time      | \n",
      " +----------------+----------------+----------------+----------------+ \n",
      " | mean:1.169e-06 | mean:2.339e-12 | mean:5.000e+00 | mean:4.800e-01 | \n",
      " | std:5.347e-07  | std:3.342e-12  | std:0.000e+00  | std:2.939e-02  | \n",
      " +----------------+----------------+----------------+----------------+ \n",
      "Sample Solution:  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_size = 20\n",
    "N = np.random.uniform(-20, 20, (sample_size, 10))\n",
    "for func in [test_func1, test_func2]:\n",
    "    solution, table = test_problem1(func, gradient(func), array(h1), N, sample_size)\n",
    "    print(table)\n",
    "    print('Sample Solution: ', *solution)\n",
    "    print() # empty line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the exact same solution and function values as like we are doing unconstrained optimization because the optimal point for both functions are happened to be on the constraint, therefore it didn't stop them converging to the minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Quadratic s.t.                             \n",
      "       x0 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 - 10 = 0        \n",
      "                              x0 - 2 = 0                               \n",
      "-----------------------------------------------------------------------\n",
      " +----------------+----------------+----------------+----------------+ \n",
      " |    ||h(x)||    |      f(x)      |   Iteration    |      time      | \n",
      " +----------------+----------------+----------------+----------------+ \n",
      " | mean:8.446e-05 | mean:1.111e+00 | mean:8.000e+00 | mean:6.220e-01 | \n",
      " | std:3.047e-07  | std:8.480e-07  | std:0.000e+00  | std:2.576e-02  | \n",
      " +----------------+----------------+----------------+----------------+ \n",
      "Sample Solution:  2.0, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889\n",
      "\n",
      "                            Rosenbrock s.t.                            \n",
      "       x0 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 - 10 = 0        \n",
      "                              x0 - 2 = 0                               \n",
      "-----------------------------------------------------------------------\n",
      " +----------------+----------------+----------------+----------------+ \n",
      " |    ||h(x)||    |      f(x)      |   Iteration    |      time      | \n",
      " +----------------+----------------+----------------+----------------+ \n",
      " | mean:7.116e-05 | mean:3.895e+01 | mean:1.200e+01 | mean:2.032e+00 | \n",
      " | std:3.849e-08  | std:4.058e-06  | std:0.000e+00  | std:1.331e-01  | \n",
      " +----------------+----------------+----------------+----------------+ \n",
      "Sample Solution:  2.0, 1.427, 1.178, 1.058, 0.988, 0.932, 0.86, 0.744, 0.552, 0.261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for func in [test_func1, test_func2]:\n",
    "    solution, table = test_problem1(func, gradient(func), array(h1, h2), N, sample_size)\n",
    "    print(table)\n",
    "    print('Sample Solution: ', *solution)\n",
    "    print() # empty line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the second constraint bounds the first variable to be 2. Thus the solutions are different, and it takes the method more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2  Stochastic Direct Search\n",
    "## Simulated Annealing\n",
    "$$-Source:Kochenderfer \\& Wheeler book, Algorithm 8.4 (pg.130), \"simulated\\_annealing\".$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_annealing(f, initial_x, T, p = 0.95, kmax = 1e4, tol = 1e-10):\n",
    "    start_time = clock()\n",
    "    x = initial_x.copy()\n",
    "    y = f(x)\n",
    "    x_best, y_best = x, y\n",
    "    iteration, accept, update = 0, 0, 0\n",
    "    while iteration < kmax:\n",
    "        iteration += 1\n",
    "        # update the point by random amount. Also scale this step based on current temperature\n",
    "        x_new = x + np.array([np.random.normal() for _ in range(len(x))]) * max(np.sqrt(T), 0.1)\n",
    "        y_new = f(x_new)\n",
    "        delta_y = y_new - y\n",
    "        # Accept the step if it is a downhill change, or the temperature is still high\n",
    "        if delta_y <= 0 or np.random.normal() < np.exp(-delta_y/T):\n",
    "            x, y = x_new, y_new\n",
    "            accept += 1\n",
    "        if y_new < y_best:\n",
    "            x_best, y_best = x_new, y_new ;#print(x_best, y_best)\n",
    "            update += 1\n",
    "        if T > tol: # stop cooling if it is cold enough\n",
    "            T *= p    \n",
    "    return accept/iteration, update/iteration, abs(y_best), clock() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test & Report\n",
    "The test functions are the same from Problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_problem2(test_func, initial_points, T, sample_size):\n",
    "    result = np.array([simulated_annealing(test_func, initial_points[i], T) for i in range(sample_size)])\n",
    "    \n",
    "    table = BeautifulTable()\n",
    "    table.set_style(BeautifulTable.STYLE_COMPACT)\n",
    "    table.column_headers = [test_func.name]\n",
    "    \n",
    "    report = BeautifulTable()\n",
    "    table.append_row([report])\n",
    "    report.column_headers = [\"Acceptance Rate\", \"Update Rate\", \"|Error|\\n|f(x)|\", \"time\"]\n",
    "    \n",
    "    row = []\n",
    "    for i in range(result.shape[1]):\n",
    "        m = np.mean(result[:,i])  # sample mean\n",
    "        s = np.sqrt(sum([(k - m)**2 for k in result[:,i]]) / (sample_size - 1)) # unbiased estimate of population std\n",
    "        row.append('mean:{:.3e}\\nstd:{:.3e}'.format(m,s))\n",
    "    report.append_row(row)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method will be tested on 20 randomly selected initial points. Each is a 10-dimensional point in [-20, 20].\\\n",
    "For the report, as usual, there are four things I interested in:\\\n",
    "(1) Acceptance Rate: How many steps are accepted over number of iteration.\\\n",
    "(2) Update Rate: How many steps actually make progress to the solution.\\\n",
    "(3) |f(x)|: Because test functions are deigned to have global minimum at f(x) = 0, this evaluate how well the method performs. \\\n",
    "(4) Running time: How efficient is this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Quadratic                                \n",
      "------------------------------------------------------------------------\n",
      " +-----------------+----------------+----------------+----------------+ \n",
      " | Acceptance Rate |  Update Rate   |    |Error|     |      time      | \n",
      " |                 |                |     |f(x)|     |                | \n",
      " +-----------------+----------------+----------------+----------------+ \n",
      " | mean:7.021e-01  | mean:3.837e-02 | mean:2.068e-01 | mean:3.119e-01 | \n",
      " |  std:5.371e-03  | std:8.883e-03  | std:6.911e-02  | std:7.090e-03  | \n",
      " +-----------------+----------------+----------------+----------------+ \n",
      "\n",
      "                               Rosenbrock                               \n",
      "------------------------------------------------------------------------\n",
      " +-----------------+----------------+----------------+----------------+ \n",
      " | Acceptance Rate |  Update Rate   |    |Error|     |      time      | \n",
      " |                 |                |     |f(x)|     |                | \n",
      " +-----------------+----------------+----------------+----------------+ \n",
      " | mean:7.027e-01  | mean:3.993e-02 | mean:1.538e+01 | mean:3.852e-01 | \n",
      " |  std:5.634e-03  | std:9.661e-03  | std:1.650e+01  | std:1.242e-02  | \n",
      " +-----------------+----------------+----------------+----------------+ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_size = 20\n",
    "T = 10\n",
    "N = np.random.uniform(-20, 20, (sample_size, 10))\n",
    "for func in [test_func1, test_func2]:\n",
    "    print(test_problem2(func, N, T, sample_size))\n",
    "    print() # empty line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions to the Quadratic problem are acceptable. However, the errors of Rosenbrock are large.\\\n",
    "The reason behind this can be explained by the small Update Rate. As the table suggested, a lot of steps are accepted, however, few make actual progress. It is probably too difficult to get the right descent direction in high dimensions.\\\n",
    "It may have better performance if it can have a larger maximum iteration number or a better cooling schedule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
