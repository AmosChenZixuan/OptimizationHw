{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zixuan Chen 61665307"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "from time import perf_counter as clock\n",
    "from beautifultable import BeautifulTable\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Function object is borrowed and modified from my own hw2 code. Now it does not use sympy.Symbols so it is more memory efficient  and does the same things. I use this object to save effort in calculating gradients and printing results later on in this program.\\\n",
    "Its differentiate method is implemented using sympy. It also remembers the position of variables in a vector so it is a lot easier for me to calculate the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "    def __init__(self, func_str, symbols, name = '', parameter_pos = None):\n",
    "        '''\n",
    "            (1) symbols are passed in as a string and seperated by ','\n",
    "            (2) parameter_pos remembers which variable the function should take. \n",
    "                Default is None because the original function takes all variables.\n",
    "                It is only useful for its partial derivatives functions.\n",
    "        '''\n",
    "        self.func = func_str\n",
    "        self.symbols = [s.strip() for s in symbols.split(',')]\n",
    "        self.s = len(self.symbols)\n",
    "        self.name = name\n",
    "        exec(f'self._f = lambda {symbols}: {func_str}')\n",
    "        \n",
    "        self.pos = parameter_pos\n",
    "        if not self.pos:\n",
    "            self.pos = list(range(self.s))       \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.func\n",
    "    \n",
    "    def __call__(self, variables):\n",
    "        ''' to call the function, we only take what need in the variable vector '''\n",
    "        if len(variables.shape) == 1: # (n,)\n",
    "            s = [variables[i] for i in self.pos]\n",
    "            return self._f(*s)\n",
    "        elif variables.shape[0] == 1: # (1, n)\n",
    "            s = [variables[0][i] for i in self.pos]\n",
    "            return self._f(*s)\n",
    "        raise AssertionError(f'Dimension dismatch {variables.shape}')\n",
    "    \n",
    "    def diff(self, symbol):\n",
    "        ''' differentiate using sympy, remember all the varibale positions and return a new Function object '''\n",
    "        df = str(sp.diff(self.func, symbol))  #str(sp.Function(str(sp.diff(self.func, symbol))))\n",
    "        sym, pos = [], []\n",
    "        for i, s in enumerate(self.symbols):\n",
    "            if s in df:\n",
    "                sym.append(s)\n",
    "                pos.append(i)\n",
    "        return Function(df, ','.join(sym), parameter_pos = pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper functions are also borrowed and modified from hw2. I use them to reduce potential duplicate typing later on in the program.\\\n",
    "New: \"assert_shape\", a sanity checker to ensure the dimensionality of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f:'Function') -> ['Function']:\n",
    "    ''' return a gradient vector'''\n",
    "    return np.array([f.diff(s) for s in f.symbols])# (n,)\n",
    "\n",
    "def compute_gradient(g: ['Function'], x: 'np.array') -> ['value']:\n",
    "    ''' compute the gradient value at given point'''\n",
    "    return np.array([d(x) for d in g], ndmin = 2)  # (1, n)\n",
    "\n",
    "def gradient_magnitude(g, x):\n",
    "    ''' return the length of the gradient vector at point/vector x'''\n",
    "    if len(g.shape) == 1: # (n,)\n",
    "        return np.sqrt(sum([i(x)**2 for i in g]))\n",
    "    elif g.shape[0] == 1: # (1, n)\n",
    "        return np.sqrt(sum([i(x)**2 for i in g[0]]))\n",
    "    raise AssertionError(f'Dimension dismatch {g.shape}')\n",
    "        \n",
    "def vector_length(v):\n",
    "    ''' return the length of a vector'''\n",
    "    if len(v.shape) == 1: # (n,)\n",
    "        return np.sqrt(sum([i**2 for i in v]))\n",
    "    elif v.shape[0] == 1: # (1, n)\n",
    "        return np.sqrt(sum([i**2 for i in v[0]]))\n",
    "    raise AssertionError(f'Dimension dismatch {v.shape}')\n",
    "    \n",
    "# sanity checker\n",
    "def assert_shape(np_obj, shape):\n",
    "    assert np_obj.shape == shape, str(np_obj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code of line search methods comes from hw1 and they are all provided in the Kochenderfer and Wheeler book:\\\n",
    "\"initial_min_bracket\" is from Algorithm 3.1, \"bracket_minimum\" (pg.36).\\\n",
    "\"golden_section\" is from Algorithm 3.3, \"golden_section_search\" (pg.41).\\\n",
    "\"line_search\" is from Algorithm 4.1, \"line_search\" (pg.54). It use the two methods above to compute a step size.\\\n",
    "I will use this method to compute the step size in Quasi-Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_min_bracket(f, start = 0, step = 1e-2, expand_factor = 2.0):\n",
    "    '''Algorithm 3.1'''\n",
    "    a, fa = start, f(start)\n",
    "    b, fb = start + step, f(start + step)\n",
    "    if fb > fa:\n",
    "        a, b = b, a\n",
    "        fa, fb = fb, fa\n",
    "        step = -step\n",
    "    while True:\n",
    "        c, fc = b + step, f(b + step)\n",
    "        if fc > fb:\n",
    "            return (a, c) if a <= c else (c, a)\n",
    "        a, fa, b, fb = b, fb, c, fc\n",
    "        step *= expand_factor\n",
    "        \n",
    "def golden_section(func, a, b, tol = 1e-6):\n",
    "    '''Algorithm 3.3'''\n",
    "    phi = (1 + np.sqrt(5))/2 - 1\n",
    "    d = phi * (b - a) + a  \n",
    "    fd = func(d)\n",
    "    \n",
    "    while abs(b - a) > tol:\n",
    "        c = b - phi * (b - a) \n",
    "        fc = func(c)\n",
    "        if fc < fd:\n",
    "            b, d, fd = d, c, fc\n",
    "        else:\n",
    "            a, b = b, c\n",
    "    return (a + b) / 2\n",
    "\n",
    "def line_search(f, x, direction):\n",
    "    '''Algorithm 4.1'''\n",
    "    objective = lambda alpha : f(x + alpha * direction)\n",
    "    a, b = initial_min_bracket(objective)\n",
    "    return golden_section(objective, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 Quasi-Newton method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: both DFP and BFGS are slightly modified because my vectors' dimensionality is different from what the book defined; mine are 1\\*n, but the Kochenderfer and Wheeler book use n\\*1. To resolve this difference, all I need to do is to transpose every vector except Q in the original formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 6.3. The Davidon-Fletcher-Powell descent method. Kochenderfer and Wheeler, pg.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DFP_method(Q: 'np.array', delta: 'np.array', gama: 'np.array', dim = 10):\n",
    "    # do matrix operations\n",
    "    delta, gama = np.mat(delta), np.mat(gama)\n",
    "    \n",
    "    a = Q* gama.T * gama * Q ; assert_shape(a, (dim,dim))\n",
    "    b = gama * Q * gama.T ; assert_shape(b, (1,1))\n",
    "    c = delta.T * (delta) ; assert_shape(c, (dim,dim))\n",
    "    d = delta * (gama.T) ; assert_shape(d, (1,1))\n",
    "    if np.asscalar(b) != 0 and np.asscalar(d) != 0: # avoid 0 division\n",
    "        Q -= a /b + c / d\n",
    "    return np.asarray(Q) # back to array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 6.4. The Broyden-Fletcher-Goldfarb-Shanno descent method. Kochenderfer and Wheeler, pg.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS_method(Q: 'np.array', delta: 'np.array', gama: 'np.array', dim = 10):\n",
    "    # do matrix operations\n",
    "    delta, gama = np.mat(delta), np.mat(gama)\n",
    "    \n",
    "    d_g = delta * gama.T ; assert_shape(d_g, (1,1))\n",
    "    if np.asscalar(d_g) == 0: # avoid 0 division\n",
    "        return Q\n",
    "    d_g_Q = delta.T *(gama) * Q; assert_shape(d_g_Q, (dim,dim))\n",
    "    Q_g_d = Q * gama.T *(delta); assert_shape(Q_g_d, (dim,dim))\n",
    "    g_Q_g = gama * Q * (gama.T); assert_shape(g_Q_g, (1,1))\n",
    "    d_d = delta.T * (delta); assert_shape(d_d, (dim,dim))\n",
    "    \n",
    "    a = (d_g_Q + Q_g_d) / d_g\n",
    "    b = d_d/d_g + np.asscalar(g_Q_g) * d_d / d_g**2\n",
    "    return np.asarray(Q - a + b) # back to array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of quasi_newton optimization:\\\n",
    "(1) initialize\\\n",
    "(2) if the gradient magnitude is smaller than the tolerance, go to (10), otherwise continue:\\\n",
    "(3) update iteration counter\\\n",
    "(4) update direction\\\n",
    "(5) find step size using line search\\\n",
    "(6) update x\\\n",
    "(7) if the convergence is very small, or the iteration count is too large, it likely marks a failure. Report failure and break to (10), otherwise continue\\\n",
    "(8) update gradient\\\n",
    "(9) update approximated inverse Hessian, Q, using change of x and change of gradient. Then go back to (2)\\\n",
    "(10) Iteration ends. Return a result consisting of ||delta||, |f(x)|, iteration counter and running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quasi_newton(f : 'Function', initial_point : 'np.array', fprime : ['Function'] , method = BFGS_method, tol = 1e-4, dim = 10):\n",
    "    # initializations\n",
    "    start_time = clock()\n",
    "    x = initial_point.copy()\n",
    "    Q = np.identity(x.shape[1])  # inverse Hessian initially be a identity matrix\n",
    "    iteration, delta = 0, np.zeros((1,dim))\n",
    "    \n",
    "    # main loop\n",
    "    g = compute_gradient(fprime, x)\n",
    "    while vector_length(g) > tol:  # stopping criterion: gradient magnitude\n",
    "        iteration += 1\n",
    "        # update x\n",
    "        direction = -g.dot(Q)\n",
    "        assert_shape(direction, (1, dim))\n",
    "        step = line_search(f, x, direction)\n",
    "        delta = step * direction; assert_shape(delta, (1, dim))\n",
    "        x += delta\n",
    "        if vector_length(delta) < 1e-14 or iteration > 3000:  # break early when likely encountering a failure\n",
    "            print('Failure. last step size:', step, ' iteration:', iteration)\n",
    "            break\n",
    "            \n",
    "        # update Q, esitimated inverse Hessian\n",
    "        g_prev = g\n",
    "        g = compute_gradient(fprime, x)\n",
    "        gama = g - g_prev; assert_shape(gama, (1, dim))\n",
    "        Q = method(Q, delta, gama, dim); assert_shape(Q, (dim, dim))\n",
    "    return vector_length(delta), abs(f(x)), iteration, clock() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Quasi-Newton method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I define two test functions.\\\n",
    "The first one is a 10-dim version of the quadratic function (x-1)^2\\\n",
    "The second one is a 10-dim Rosenbrock function with a = 1 and b = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Function 1: Quadratic\n",
      "(x0 - 1)**2 + (x1 - 1)**2 + (x2 - 1)**2 + (x3 - 1)**2 + (x4 - 1)**2 + (x5 - 1)**2 + (x6 - 1)**2 + (x7 - 1)**2 + (x8 - 1)**2 + (x9 - 1)**2\n",
      "variables: ['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']\n",
      "\n",
      "Test Function 2: Rosenbrock\n",
      "(1 - x0)**2 + 5*(x1 - x0**2)**2 + (1 - x1)**2 + 5*(x2 - x1**2)**2 + (1 - x2)**2 + 5*(x3 - x2**2)**2 + (1 - x3)**2 + 5*(x4 - x3**2)**2 + (1 - x4)**2 + 5*(x5 - x4**2)**2 + (1 - x5)**2 + 5*(x6 - x5**2)**2 + (1 - x6)**2 + 5*(x7 - x6**2)**2 + (1 - x7)**2 + 5*(x8 - x7**2)**2 + (1 - x8)**2 + 5*(x9 - x8**2)**2\n",
      "variables: ['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10-dim (x-1)^2\n",
    "f_str = ' + '.join([f'(x{i} - 1)**2' for i in range(10)])\n",
    "test_func1 = Function(f_str, ','.join([f'x{i}' for i in range(10)]), name = 'Quadratic')\n",
    "\n",
    "# 10-dim Rosenbrock \n",
    "f_str = ' + '.join([f'(1 - x{i})**2 + 5*(x{i+1} - x{i}**2)**2' for i in range(9)])\n",
    "test_func2 = Function(f_str, ','.join([f'x{i}' for i in range(10)]), name = 'Rosenbrock')\n",
    "\n",
    "for i, tf in enumerate([test_func1, test_func2], 1):\n",
    "    print(f'Test Function {i}: {tf.name}\\n{tf}\\nvariables: {tf.symbols}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test function defined below will optimize a given function over N starting points, using a given method, and report the results using a BeautifulTable.\\\n",
    "The table include a sample mean and an unbiased estimate of population std for each of Convergence Measure, absolute error from the global solution, iteration number and running time.\\\n",
    "In this problem, I set N = 50 and each point is a 10-dim vector ranging from -100 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(func, init_pt, func_gradient, method, sample_size):\n",
    "    result = np.array([quasi_newton(func, init_pt[i], func_gradient, method) for i in range(sample_size)])\n",
    "    \n",
    "    table = BeautifulTable()\n",
    "    table.set_style(BeautifulTable.STYLE_COMPACT)\n",
    "    table.column_headers = [func.name]\n",
    "    \n",
    "    report = BeautifulTable()\n",
    "    table.append_row([report])\n",
    "    report.column_headers = [\"Convergence Measure\\n||x` - x||\", \"|error|\\n|f(x)|\", \"Iteration\", \"time\"]\n",
    "    \n",
    "    row = []\n",
    "    for i in range(result.shape[1]):\n",
    "        m = np.mean(result[:,i])  # sample mean\n",
    "        s = np.sqrt(sum([(k - m)**2 for k in result[:,i]]) / (sample_size - 1)) # unbiased estimate of population std\n",
    "        row.append('mean:{:.3e}\\nstd:{:.3e}'.format(m,s))\n",
    "    report.append_row(row)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method used:  DFP_method\n",
      "                                 Quadratic                                  \n",
      "----------------------------------------------------------------------------\n",
      " +---------------------+----------------+----------------+----------------+ \n",
      " | Convergence Measure |    |error|     |   Iteration    |      time      | \n",
      " |     ||x` - x||      |     |f(x)|     |                |                | \n",
      " +---------------------+----------------+----------------+----------------+ \n",
      " |   mean:1.819e+02    | mean:1.415e-10 | mean:1.000e+00 | mean:6.931e-04 | \n",
      " |    std:3.043e+01    | std:4.403e-11  | std:0.000e+00  | std:3.131e-04  | \n",
      " +---------------------+----------------+----------------+----------------+ \n",
      "\n",
      "Method used:  DFP_method\n",
      "                                 Rosenbrock                                 \n",
      "----------------------------------------------------------------------------\n",
      " +---------------------+----------------+----------------+----------------+ \n",
      " | Convergence Measure |    |error|     |   Iteration    |      time      | \n",
      " |     ||x` - x||      |     |f(x)|     |                |                | \n",
      " +---------------------+----------------+----------------+----------------+ \n",
      " |   mean:2.445e-05    | mean:8.164e-01 | mean:8.096e+01 | mean:8.573e-02 | \n",
      " |    std:1.693e-05    | std:1.553e+00  | std:1.468e+01  | std:1.764e-02  | \n",
      " +---------------------+----------------+----------------+----------------+ \n",
      "\n",
      "Method used:  BFGS_method\n",
      "                                 Quadratic                                  \n",
      "----------------------------------------------------------------------------\n",
      " +---------------------+----------------+----------------+----------------+ \n",
      " | Convergence Measure |    |error|     |   Iteration    |      time      | \n",
      " |     ||x` - x||      |     |f(x)|     |                |                | \n",
      " +---------------------+----------------+----------------+----------------+ \n",
      " |   mean:1.819e+02    | mean:1.415e-10 | mean:1.000e+00 | mean:6.509e-04 | \n",
      " |    std:3.043e+01    | std:4.403e-11  | std:0.000e+00  | std:6.026e-05  | \n",
      " +---------------------+----------------+----------------+----------------+ \n",
      "\n",
      "Method used:  BFGS_method\n",
      "                                 Rosenbrock                                 \n",
      "----------------------------------------------------------------------------\n",
      " +---------------------+----------------+----------------+----------------+ \n",
      " | Convergence Measure |    |error|     |   Iteration    |      time      | \n",
      " |     ||x` - x||      |     |f(x)|     |                |                | \n",
      " +---------------------+----------------+----------------+----------------+ \n",
      " |   mean:3.159e-05    | mean:5.937e-01 | mean:8.286e+01 | mean:7.885e-02 | \n",
      " |    std:3.337e-05    | std:1.374e+00  | std:1.741e+01  | std:1.667e-02  | \n",
      " +---------------------+----------------+----------------+----------------+ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_size = 50\n",
    "N = np.random.uniform(-100, 100, (sample_size, 1, 10))\n",
    "\n",
    "for method in [DFP_method, BFGS_method]:\n",
    "    for func in [test_func1, test_func2]:\n",
    "        print('Method used: ', method.__name__)\n",
    "        print(test(func, N, gradient(func), method, sample_size))\n",
    "        print() # empty line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\\\n",
    "Because the direction is not normalized, the Quasi-Newton works very well on the quadratic function. It takes only one step to reach a global minimum.\\\n",
    "It has a relatively large error on the Rosenbrock's function because in high dimensions (d > 3), depending on the starting point, it might converge to a local minimum near (-1, 1, ... 1) (Source: Piazza@24 and Wikipedia).\\\n",
    "Overall, BFGS has better performance than DFP; it takes fewer iterations to converge and cost less running time. But the solutions they produce are similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
